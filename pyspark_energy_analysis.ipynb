{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Data Engineering: Energy Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of PySpark for data engineering tasks, transitioning from traditional databases like PostgreSQL and MongoDB to distributed data processing with PySpark. The goal is to showcase proficiency in data loading, exploration, transformation, and analysis using PySpark, as well as to highlight the scalability and efficiency of distributed computing for large datasets.\n",
    "\n",
    "We will use datasets from [Open Power System Data (OPSD)](https://open-power-system-data.org/) to perform exploratory data analysis (EDA), data transformations, and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the time_series_60min_singleindex.csv dataset from OPSD, which contains hourly time series data for power systems in Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL and save path\n",
    "url = \"https://data.open-power-system-data.org/time_series/2020-10-06/time_series_60min_singleindex.csv\"\n",
    "save_path = \"data/raw/time_series_60min_singleindex.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at C:\\slPrivateData\\00_portfolio\\pyspark_energy_analysi\\data\\raw\\time_series_60min_singleindex.csv. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def download_data(url, save_path):\n",
    "    try:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        # Convert the save path to a canonical path\n",
    "        save_path = Path(save_path).resolve()\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"File already exists at {save_path}. Skipping download.\")\n",
    "            return\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "        # Save the file\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        \n",
    "        print(f\"Data downloaded successfully and saved to {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading data: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Download the data\n",
    "download_data(url, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains hourly time series data for:\n",
    "\n",
    "- Renewable energy generation (solar, wind, etc.)\n",
    "\n",
    "- Conventional power generation\n",
    "\n",
    "- Electricity consumption\n",
    "\n",
    "- Cross-border electricity exchanges\n",
    "\n",
    "This data is crucial for analyzing energy trends and optimizing power systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the downloaded CSV file into a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def initialize_spark_session(app_name: str, master: str = \"local[*]\", **kwargs) -> SparkSession:\n",
    "    \"\"\"\n",
    "    Initialize a Spark session with the given configuration.\n",
    "\n",
    "    Args:\n",
    "        app_name (str): The name of the Spark application.\n",
    "        master (str, optional): The Spark master URL. Defaults to \"local[*]\".\n",
    "        **kwargs: Additional configuration options.\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: The initialized Spark session.\n",
    "    \"\"\"\n",
    "    # Stop any existing Spark session\n",
    "    if 'spark' in globals():\n",
    "        spark.stop()\n",
    "\n",
    "    # Initialize Spark session\n",
    "    return SparkSession.builder.appName(app_name) \\\n",
    "        .master(master) \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.instances\", \"1\") \\\n",
    "        .config(\"spark.ui.port\", \"4041\") \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "        .config(\"spark.network.timeout\", \"800s\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.default.parallelism\", \"100\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = initialize_spark_session(\"EnergyAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession:\n",
      "----------------\n",
      "SparkContext: <SparkContext master=local[*] appName=EnergyAnalysis>\n",
      "Spark UI: http://ERENL004:4041\n",
      "Version: 3.5.5\n",
      "Master: local[*]\n",
      "AppName: EnergyAnalysis\n"
     ]
    }
   ],
   "source": [
    "# Print SparkSession information\n",
    "print(\"SparkSession:\")\n",
    "print(\"----------------\")\n",
    "print(f\"SparkContext: {spark.sparkContext}\")\n",
    "print(\"Spark UI: {}\".format(spark.sparkContext.uiWebUrl))\n",
    "print(f\"Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"AppName: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ERENL004:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>EnergyAnalysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x165e5d4c6e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.startTime: 1743000394283\n",
      "spark.app.name: EnergyAnalysis\n",
      "spark.app.submitTime: 1743000394169\n",
      "spark.executor.cores: 4\n",
      "spark.driver.port: 50147\n",
      "spark.ui.port: 4041\n",
      "spark.driver.memory: 8g\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.sql.warehouse.dir: file:/C:/slPrivateData/00_portfolio/pyspark_energy_analysi/spark-warehouse\n",
      "spark.sql.shuffle.partitions: 200\n",
      "spark.master: local[*]\n",
      "spark.submit.deployMode: client\n",
      "spark.sql.execution.arrow.pyspark.enabled: true\n",
      "spark.app.id: local-1743000395306\n",
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.default.parallelism: 100\n",
      "spark.executor.heartbeatInterval: 60s\n",
      "spark.driver.host: ERENL004\n",
      "spark.executor.memory: 4g\n",
      "spark.sql.adaptive.enabled: true\n",
      "spark.executor.id: driver\n",
      "spark.network.timeout: 800s\n",
      "spark.rdd.compress: True\n",
      "spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.instances: 1\n",
      "spark.submit.pyFiles: \n",
      "spark.ui.showConsoleProgress: true\n",
      "spark.driver.maxResultSize: 4g\n"
     ]
    }
   ],
   "source": [
    "# Get all configuration properties of the Spark context\n",
    "conf = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# Print configuration properties\n",
    "for key, value in conf:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(spark, file_path, options):\n",
    "    \"\"\"\n",
    "    Load a CSV file using PySpark with specified options\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file\n",
    "    options : dict\n",
    "        Options for loading the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    spark.DataFrame\n",
    "    \"\"\"\n",
    "    return spark.read.options(**options).csv(file_path)\n",
    "\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "\n",
    "def get_numerical_columns(df):\n",
    "    \"\"\"\n",
    "    Get the list of numerical columns in the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.dataframe.DataFrame\n",
    "        The DataFrame to get numerical columns from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of numerical column names.\n",
    "    \"\"\"\n",
    "    return [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "\n",
    "def get_categorical_columns(df):\n",
    "    \"\"\"\n",
    "    Get the list of categorical columns in the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.dataframe.DataFrame\n",
    "        The DataFrame to get categorical columns from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of categorical column names.\n",
    "    \"\"\"\n",
    "    return [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "def get_timestamp_columns(df):\n",
    "    return [col for col in df.columns \n",
    "                     if any(time_keyword in col.lower() \n",
    "                           for time_keyword in [\"time\", \"date\", \"timestamp\", \"hour\", \"day\"])]\n",
    "\n",
    "def create_sample(df, sampling_ratio, seed):\n",
    "    \"\"\"\n",
    "    Create a smaller sample of a DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : spark.DataFrame\n",
    "        DataFrame to sample\n",
    "    sampling_ratio : float\n",
    "        Ratio for sampling data (between 0 and 1)\n",
    "    seed : int\n",
    "        Seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    spark.DataFrame\n",
    "    \"\"\"\n",
    "    return df.sample(withReplacement=False, fraction=sampling_ratio, seed=seed)\n",
    "\n",
    "def cache_data(df):\n",
    "    \"\"\"\n",
    "    Cache a DataFrame for faster subsequent operations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : spark.DataFrame\n",
    "        DataFrame to cache\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    spark.DataFrame\n",
    "    \"\"\"\n",
    "    df.cache()\n",
    "    return df\n",
    "\n",
    "def create_metadata(df):\n",
    "    \"\"\"\n",
    "    Print information about a DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : spark.DataFrame\n",
    "        DataFrame to print information about\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\"rows\": df.count(),\n",
    "            \"columns\": len(df.columns),\n",
    "            \"timestamp_col\": get_timestamp_columns(df),\n",
    "            \"numeric_cols\": get_numerical_columns(df),\n",
    "            \"categorical_cols\": get_categorical_columns(df)}\n",
    "    print(f\"Dataset size: {df.count():,} Records - {len(df.columns):,} Features\")\n",
    "\n",
    "def load_data(spark, file_path, cache=True):\n",
    "    \"\"\"\n",
    "    Load a large CSV file efficiently using PySpark\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file\n",
    "    sampling_ratio : float\n",
    "        Ratio for sampling data (between 0 and 1)\n",
    "    cache : bool\n",
    "        Whether to cache the DataFrame for faster subsequent operations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (full_df, sampled_df)\n",
    "        full_df: Full PySpark DataFrame\n",
    "        sampled_df: Sampled PySpark DataFrame for quick exploration\n",
    "    \"\"\"\n",
    "    options = {\n",
    "        \"header\": \"true\",\n",
    "        \"inferSchema\": \"true\",\n",
    "        \"mode\": \"DROPMALFORMED\"\n",
    "    }\n",
    "    \n",
    "    df = load_csv(spark, file_path, options)\n",
    "    \n",
    "    if cache:\n",
    "        df = cache_data(df)\n",
    "    \n",
    "    return df, create_metadata(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, metadata = load_data(spark, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- InMemoryTableScan [utc_timestamp#17, cet_cest_timestamp#18, AT_load_actual_entsoe_transparency#19, AT_load_forecast_entsoe_transparency#20, AT_price_day_ahead#21, AT_solar_generation_actual#22, AT_wind_onshore_generation_actual#23, BE_load_actual_entsoe_transparency#24, BE_load_forecast_entsoe_transparency#25, BE_solar_generation_actual#26, BE_wind_generation_actual#27, BE_wind_offshore_generation_actual#28, BE_wind_onshore_generation_actual#29, BG_load_actual_entsoe_transparency#30, BG_load_forecast_entsoe_transparency#31, BG_solar_generation_actual#32, BG_wind_onshore_generation_actual#33, CH_load_actual_entsoe_transparency#34, CH_load_forecast_entsoe_transparency#35, CH_solar_capacity#36, CH_solar_generation_actual#37, CH_wind_onshore_capacity#38, CH_wind_onshore_generation_actual#39, CY_load_actual_entsoe_transparency#40, ... 276 more fields]\n",
      "      +- InMemoryRelation [utc_timestamp#17, cet_cest_timestamp#18, AT_load_actual_entsoe_transparency#19, AT_load_forecast_entsoe_transparency#20, AT_price_day_ahead#21, AT_solar_generation_actual#22, AT_wind_onshore_generation_actual#23, BE_load_actual_entsoe_transparency#24, BE_load_forecast_entsoe_transparency#25, BE_solar_generation_actual#26, BE_wind_generation_actual#27, BE_wind_offshore_generation_actual#28, BE_wind_onshore_generation_actual#29, BG_load_actual_entsoe_transparency#30, BG_load_forecast_entsoe_transparency#31, BG_solar_generation_actual#32, BG_wind_onshore_generation_actual#33, CH_load_actual_entsoe_transparency#34, CH_load_forecast_entsoe_transparency#35, CH_solar_capacity#36, CH_solar_generation_actual#37, CH_wind_onshore_capacity#38, CH_wind_onshore_generation_actual#39, CY_load_actual_entsoe_transparency#40, ... 276 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- FileScan csv [utc_timestamp#17,cet_cest_timestamp#18,AT_load_actual_entsoe_transparency#19,AT_load_forecast_entsoe_transparency#20,AT_price_day_ahead#21,AT_solar_generation_actual#22,AT_wind_onshore_generation_actual#23,BE_load_actual_entsoe_transparency#24,BE_load_forecast_entsoe_transparency#25,BE_solar_generation_actual#26,BE_wind_generation_actual#27,BE_wind_offshore_generation_actual#28,BE_wind_onshore_generation_actual#29,BG_load_actual_entsoe_transparency#30,BG_load_forecast_entsoe_transparency#31,BG_solar_generation_actual#32,BG_wind_onshore_generation_actual#33,CH_load_actual_entsoe_transparency#34,CH_load_forecast_entsoe_transparency#35,CH_solar_capacity#36,CH_solar_generation_actual#37,CH_wind_onshore_capacity#38,CH_wind_onshore_generation_actual#39,CY_load_actual_entsoe_transparency#40,... 276 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/slPrivateData/00_portfolio/pyspark_energy_analysi/data/raw/ti..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<utc_timestamp:timestamp,cet_cest_timestamp:timestamp,AT_load_actual_entsoe_transparency:do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.types import StructField\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "def validate_df(df: SparkDataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Validate if the input is a PySpark DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: The input to validate\n",
    "    \n",
    "    Raises:\n",
    "        TypeError: if the df is not a spark dataframe\n",
    "    \"\"\"\n",
    "    if not isinstance(df, SparkDataFrame):\n",
    "        raise TypeError(\"Input 'df' must be a PySpark DataFrame.\")\n",
    "\n",
    "def validate_output_dir(output_dir: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Validate if the output directory is a valid path\n",
    "    \n",
    "    Args:\n",
    "        output_dir: The directory to validate\n",
    "    \n",
    "    Returns:\n",
    "        str: The validated output directory\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: if the output_dir is not a valid path\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = os.getcwd()\n",
    "    if not os.path.isdir(output_dir):\n",
    "        raise ValueError(f\"output_dir: {output_dir} is not a directory\")\n",
    "    return output_dir\n",
    "\n",
    "def extract_schema_info(df: SparkDataFrame) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract schema information from a PySpark DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: The PySpark DataFrame to extract schema from\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries containing column name and data type\n",
    "    \"\"\"\n",
    "    schema_info = []\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field, StructField):\n",
    "            schema_info.append({\"Column Name\": field.name, \"Data Type\": str(field.dataType)})\n",
    "        else:\n",
    "            raise TypeError(\"Each field in the df.schema.fields must be a StructField\")\n",
    "    return schema_info\n",
    "\n",
    "def print_schema_info(schema_info: List[Dict[str, str]]) -> None:\n",
    "    \"\"\"\n",
    "    Print schema information\n",
    "    \n",
    "    Args:\n",
    "        schema_info: A list of dictionaries containing column name and data type\n",
    "    \"\"\"\n",
    "    print(\"\\nDataFrame Structure:\")\n",
    "    print(pd.DataFrame(schema_info))\n",
    "\n",
    "def save_schema_info(schema_info: List[Dict[str, str]], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Save schema information to a CSV file\n",
    "    \n",
    "    Args:\n",
    "        schema_info: A list of dictionaries containing column name and data type\n",
    "        output_dir: The directory to save the schema information CSV\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(output_dir, \"schema_info.csv\")\n",
    "    pd.DataFrame(schema_info).to_csv(output_path, index=False)\n",
    "    print(f\"Schema information saved to: {output_path}\")\n",
    "\n",
    "def explore_schema(df: SparkDataFrame, output_dir: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Explores and documents the schema of a PySpark DataFrame.\n",
    "    \n",
    "    Prints the schema to the console and saves a structured representation\n",
    "    (column name and data type) to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df: The PySpark DataFrame to explore.\n",
    "        output_dir: The directory to save the schema information CSV.\n",
    "                    Defaults to the current working directory if not provided.\n",
    "    \"\"\"\n",
    "    validate_df(df)\n",
    "    output_dir = validate_output_dir(output_dir)\n",
    "    \n",
    "    print(\"DataFrame Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    schema_info = extract_schema_info(df)\n",
    "    print_schema_info(schema_info)\n",
    "    save_schema_info(schema_info, output_dir)\n",
    "    return schema_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_info = explore_schema(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "full_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a dataframe with only the columns that contain keywords from a list\n",
    "def filter_columns(df, keywords):\n",
    "    return df.select([col for col in df.columns if any(keyword.lower() in col.lower() for keyword in keywords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_de = filter_columns(full_df, [\"utc\",\"DE\"])\n",
    "full_df_de.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Sample Size for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Sample Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Define Sampling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_sample_size(\n",
    "    confidence_level: float,\n",
    "    margin_of_error: float,\n",
    "    population_size: int = None,\n",
    "    std_dev: float = 0.5\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Calculate the required sample size for a given confidence level and margin of error,\n",
    "    with optional finite population correction (FPC).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    confidence_level : float\n",
    "        Confidence level (e.g., 0.90 for 90%)\n",
    "    margin_of_error : float\n",
    "        Margin of error (e.g., 0.10 for Â±10%)\n",
    "    population_size : int, optional\n",
    "        Total population size. If provided, applies finite population correction.\n",
    "    std_dev : float, optional\n",
    "        Standard deviation (default 0.5 for maximum variability).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Required sample size\n",
    "    \"\"\"\n",
    "    # Z-scores for common confidence levels\n",
    "    z_scores = {\n",
    "        0.80: 1.28,\n",
    "        0.85: 1.44,\n",
    "        0.90: 1.645,\n",
    "        0.95: 1.96,\n",
    "        0.99: 2.58\n",
    "    }\n",
    "    \n",
    "    # Handle missing confidence level\n",
    "    if confidence_level not in z_scores:\n",
    "        closest_key = min(z_scores.keys(), key=lambda x: abs(x - confidence_level))\n",
    "        print(f\"Warning: Confidence level {confidence_level} not found. Using closest: {closest_key}\")\n",
    "        z = z_scores[closest_key]\n",
    "    else:\n",
    "        z = z_scores[confidence_level]\n",
    "    \n",
    "    # Calculate initial sample size (Cochran's formula)\n",
    "    n = ((z ** 2) * std_dev * (1 - std_dev)) / (margin_of_error ** 2)\n",
    "    \n",
    "    # Apply finite population correction if population_size is provided\n",
    "    if population_size is not None and n > 0:\n",
    "        n = n / (1 + (n - 1) / population_size)\n",
    "    \n",
    "    return math.ceil(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Sampling Strategies Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# 3.1 Random Sampling\n",
    "def random_sampling(data, sample_size):\n",
    "    \"\"\"Perform random sampling on the dataset\"\"\"\n",
    "    return data.sample(fraction=sample_size/data.count(), seed=42)\n",
    "\n",
    "# 3.2 Systematic Sampling - take every nth record\n",
    "def systematic_sampling(data: DataFrame, sample_size: int, timestamp_column: str = \"utc_timestamp\") -> DataFrame:\n",
    "    \"\"\"Perform systematic sampling on the dataset\"\"\"\n",
    "    \n",
    "    # Calculate the total number of records and step size\n",
    "    total_records = data.count()\n",
    "    n = total_records // sample_size\n",
    "    print(f\"Total records: {total_records}, Sample size: {sample_size}, Step size: {n}\")\n",
    "    \n",
    "    # Order the data by the timestamp column\n",
    "    ordered_data = data.orderBy(timestamp_column)\n",
    "    \n",
    "    # Add an index column to the DataFrame\n",
    "    indexed_data = ordered_data.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "    \n",
    "    # Filter the rows based on the index\n",
    "    sampled_data = indexed_data.filter((F.col(\"index\") % n) == 0).drop(\"index\")\n",
    "    \n",
    "    return sampled_data\n",
    "\n",
    "# 3.3 Stratified Sampling based on time periods\n",
    "\n",
    "def stratified_time_sampling(data, sample_size, timestamp_column: str = \"utc_timestamp\"):\n",
    "    \"\"\"Perform stratified sampling based on time periods\"\"\"\n",
    "\n",
    "    # Add time period columns (hour, day, month)\n",
    "    data_with_periods = data.withColumn(\"hour\", F.hour(timestamp_column)) \\\n",
    "                            .withColumn(\"day\", F.dayofweek(timestamp_column)) \\\n",
    "                            .withColumn(\"month\", F.month(timestamp_column))\n",
    "\n",
    "    # Create a composite stratum key\n",
    "    data_with_periods = data_with_periods.withColumn(\"stratum_key\", \n",
    "                                                     F.concat_ws(\"_\", \n",
    "                                                                 F.col(\"month\"), \n",
    "                                                                 F.col(\"day\"), \n",
    "                                                                 F.col(\"hour\")))\n",
    "\n",
    "    # Get counts for each stratum\n",
    "    strata_counts = data_with_periods.groupBy(\"stratum_key\").count().collect()\n",
    "    total_count = data.count()\n",
    "\n",
    "    # Calculate sampling fractions for each stratum\n",
    "    strata_fractions = {row[\"stratum_key\"]: min(sample_size / total_count, 1.0) for row in strata_counts}\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    sampled_data = data_with_periods.sampleBy(\"stratum_key\", strata_fractions, seed=42)\n",
    "\n",
    "    # If we got more than needed, take a subsample\n",
    "    if sampled_data.count() > sample_size:\n",
    "        sampled_data = sampled_data.sample(fraction=sample_size / sampled_data.count(), seed=42)\n",
    "\n",
    "    # Drop the added columns\n",
    "    sampled_data = sampled_data.drop(\"hour\", \"day\", \"month\", \"stratum_key\")\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "# 3.4 Time-based Clustering Sampling\n",
    "def cluster_sampling(data, sample_size, num_clusters=10, timestamp_column=\"utc_timestamp\"):\n",
    "    \"\"\"\n",
    "    Perform time-based cluster sampling\n",
    "    \n",
    "    First divide the time range into clusters (time windows),\n",
    "    then sample from each cluster\n",
    "    \"\"\"\n",
    "    # Get min and max timestamps\n",
    "    min_max_times = data.agg(\n",
    "        F.min(timestamp_column).alias(\"min_time\"),\n",
    "        F.max(timestamp_column).alias(\"max_time\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    min_time = min_max_times[\"min_time\"]\n",
    "    max_time = min_max_times[\"max_time\"]\n",
    "    \n",
    "    # Calculate time range and cluster interval\n",
    "    from datetime import timedelta\n",
    "    total_seconds = (max_time - min_time).total_seconds()\n",
    "    interval_seconds = total_seconds / num_clusters\n",
    "    \n",
    "    # Generate cluster boundaries\n",
    "    cluster_boundaries = [min_time + timedelta(seconds=i*interval_seconds) for i in range(num_clusters+1)]\n",
    "    \n",
    "    # Initialize empty DataFrame for results\n",
    "    sampled_data = None\n",
    "    samples_per_cluster = sample_size // num_clusters\n",
    "    \n",
    "    # Sample from each cluster\n",
    "    for i in range(num_clusters):\n",
    "        cluster_data = data.filter(\n",
    "            (F.col(timestamp_column) >= cluster_boundaries[i]) & \n",
    "            (F.col(timestamp_column) < cluster_boundaries[i+1])\n",
    "        )\n",
    "        \n",
    "        # Skip empty clusters\n",
    "        if cluster_data.count() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Sample from this cluster\n",
    "        cluster_sample = cluster_data.sample(\n",
    "            fraction=min(1.0, samples_per_cluster/cluster_data.count()), \n",
    "            seed=42+i\n",
    "        )\n",
    "        \n",
    "        # Add to the result\n",
    "        if sampled_data is None:\n",
    "            sampled_data = cluster_sample\n",
    "        else:\n",
    "            sampled_data = sampled_data.union(cluster_sample)\n",
    "    \n",
    "    return sampled_data\n",
    "\n",
    "# 3.5 Reservoir Sampling for streams or very large datasets\n",
    "def reservoir_sampling_simulation(data, sample_size):\n",
    "    \"\"\"\n",
    "    Simulate reservoir sampling which is useful for streaming data\n",
    "    \n",
    "    This is a simulation because in a notebook we already have all the data.\n",
    "    In a real streaming scenario, this would be implemented differently.\n",
    "    \"\"\"\n",
    "    # For simulation purposes, we'll use Spark's sample function\n",
    "    return data.sample(fraction=sample_size/data.count(), seed=42)\n",
    "\n",
    "# 3.6 Adaptive Sampling based on data variance\n",
    "def adaptive_sampling(data, base_sample_size, max_sample_size):\n",
    "    \"\"\"\n",
    "    Implement adaptive sampling based on data variance\n",
    "    \n",
    "    We first take a small sample and compute the coefficient of variation.\n",
    "    Then we adjust the sample size based on the variance.\n",
    "    \"\"\"\n",
    "    # Take an initial sample\n",
    "    initial_sample = data.sample(fraction=base_sample_size/data.count(), seed=42)\n",
    "    \n",
    "    # Calculate coefficient of variation for some key columns\n",
    "    # For simplicity, we'll check variance in temperature and energy sensors\n",
    "    temp_cols = [col for col in data.columns if \"temperature\" in col][:5]  # Just use a few columns\n",
    "    energy_cols = [col for col in data.columns if \"energy\" in col][:5]\n",
    "    \n",
    "    # Function to calculate coefficient of variation (CV)\n",
    "    def calculate_cv(df, col):\n",
    "        stats_df = df.select(\n",
    "            F.mean(col).alias(\"mean\"),\n",
    "            F.stddev(col).alias(\"stddev\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        mean = stats_df[\"mean\"]\n",
    "        stddev = stats_df[\"stddev\"]\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if mean == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        return stddev / abs(mean)\n",
    "    \n",
    "    # Calculate average CV for the selected columns\n",
    "    cv_values = []\n",
    "    for col in temp_cols + energy_cols:\n",
    "        cv = calculate_cv(initial_sample, col)\n",
    "        if not math.isinf(cv):\n",
    "            cv_values.append(cv)\n",
    "    \n",
    "    avg_cv = sum(cv_values) / len(cv_values) if cv_values else 1.0\n",
    "    \n",
    "    # Adjust sample size based on CV\n",
    "    # Higher CV means more variance, which requires larger sample\n",
    "    adjustment_factor = min(max(avg_cv, 0.5), 5.0)  # Limit between 0.5 and 5\n",
    "    adjusted_sample_size = min(int(base_sample_size * adjustment_factor), max_sample_size)\n",
    "    \n",
    "    print(f\"Average Coefficient of Variation: {avg_cv:.4f}\")\n",
    "    print(f\"Adjustment Factor: {adjustment_factor:.4f}\")\n",
    "    print(f\"Adjusted Sample Size: {adjusted_sample_size} (from base of {base_sample_size})\")\n",
    "    \n",
    "    # Take the adjusted sample\n",
    "    return data.sample(fraction=adjusted_sample_size/data.count(), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Evaluate and Compare Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set desired parameters\n",
    "CONFIDENCE_LEVEL = 0.95  # 95% confidence level\n",
    "MARGIN_OF_ERROR = 0.01   # 3% margin of error\n",
    "\n",
    "# Calculate required sample size based on confidence level and margin of error\n",
    "sample_size = calculate_sample_size(CONFIDENCE_LEVEL, MARGIN_OF_ERROR)\n",
    "print(f\"Required sample size for {CONFIDENCE_LEVEL*100}% confidence level and {MARGIN_OF_ERROR*100}% margin of error: {sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply different sampling methods\n",
    "print(\"\\nApplying sampling methods...\")\n",
    "random_sample = random_sampling(full_df_de, sample_size)\n",
    "systematic_sample = systematic_sampling(full_df_de, sample_size)\n",
    "stratified_sample = stratified_time_sampling(full_df_de, sample_size)\n",
    "cluster_sample = cluster_sampling(full_df_de, sample_size)\n",
    "reservoir_sample = reservoir_sampling_simulation(full_df_de, sample_size)\n",
    "adaptive_sample = adaptive_sampling(full_df_de, sample_size, sample_size*10)\n",
    "\n",
    "# Cache the samples for repeated use\n",
    "random_sample.cache()\n",
    "systematic_sample.cache()\n",
    "stratified_sample.cache()\n",
    "cluster_sample.cache()\n",
    "reservoir_sample.cache()\n",
    "adaptive_sample.cache()\n",
    "\n",
    "# Get actual sample sizes\n",
    "print(f\"Random Sample Size: {random_sample.count()}\")\n",
    "print(f\"Systematic Sample Size: {systematic_sample.count()}\")\n",
    "print(f\"Stratified Sample Size: {stratified_sample.count()}\")\n",
    "print(f\"Cluster Sample Size: {cluster_sample.count()}\")\n",
    "print(f\"Reservoir Sample Size: {reservoir_sample.count()}\")\n",
    "print(f\"Adaptive Sample Size: {adaptive_sample.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Plot Sampled Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sample DataFrames and their corresponding titles\n",
    "sample_dataframes = [\n",
    "    (random_sample, \"Random Sample\"),\n",
    "    (systematic_sample, \"Systematic Sample\"),\n",
    "    (stratified_sample, \"Stratified Sample\"),\n",
    "    (cluster_sample, \"Cluster Sample\"),\n",
    "    (reservoir_sample, \"Reservoir Sample\"),\n",
    "    (adaptive_sample, \"Adaptive Sample\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the name of a pandas dataframe\n",
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that will plot a line chart of a given column timeseries in the sample dataframes\n",
    "def plot_line_chart(df, index, column):\n",
    "    df.toPandas()[[index, column]].plot(x=index, y=column, title=get_df_name(df), figsize=(12, 6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _df, _title in sample_dataframes:\n",
    "    plot_line_chart(_df, \"utc_timestamp\", \"DE_load_actual_entsoe_transparency\")\n",
    "\n",
    "plot_line_chart(full_df_de, \"utc_timestamp\", \"DE_load_actual_entsoe_transparency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Sampling Strategies Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute basic statistics for a specific column\n",
    "def compute_column_stats(df, column):\n",
    "    \"\"\"Compute basic statistics for a column\"\"\"\n",
    "    stats = df.select(\n",
    "        F.min(column).alias(\"min\"),\n",
    "        F.max(column).alias(\"max\"),\n",
    "        F.mean(column).alias(\"mean\"),\n",
    "        F.stddev(column).alias(\"stddev\"),\n",
    "        F.skewness(column).alias(\"skewness\"),\n",
    "        F.kurtosis(column).alias(\"kurtosis\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    return {\n",
    "        \"min\": stats[\"min\"],\n",
    "        \"max\": stats[\"max\"],\n",
    "        \"mean\": stats[\"mean\"],\n",
    "        \"stddev\": stats[\"stddev\"],\n",
    "        \"skewness\": stats[\"skewness\"],\n",
    "        \"kurtosis\": stats[\"kurtosis\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few columns for comparison\n",
    "# selected_columns = [\n",
    "#     \"DE_load_actual_entsoe_transparency\"\n",
    "# ]\n",
    "\n",
    "selected_columns = [col for col in full_df_de.columns if col != \"utc_timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for the full dataset\n",
    "full_data_stats = {}\n",
    "for col in selected_columns:\n",
    "    full_data_stats[col] = compute_column_stats(full_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate error percentage\n",
    "def calculate_error(sample_stat, full_stat):\n",
    "    \"\"\"Calculate percentage error\"\"\"\n",
    "    if full_stat == 0:\n",
    "        return float('inf')\n",
    "    return 100 * abs(sample_stat - full_stat) / abs(full_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data structure for comparison\n",
    "comparison_results = {}\n",
    "\n",
    "# Define the dataframes to compare\n",
    "samples = {\n",
    "    \"Random\": random_sample,\n",
    "    \"Systematic\": systematic_sample,\n",
    "    \"Stratified\": stratified_sample,\n",
    "    \"Cluster\": cluster_sample,\n",
    "    \"Reservoir\": reservoir_sample,\n",
    "    \"Adaptive\": adaptive_sample\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and compare statistics\n",
    "for sample_name, sample_df in samples.items():\n",
    "    comparison_results[sample_name] = {}\n",
    "    \n",
    "    for col in selected_columns:\n",
    "        sample_stats = compute_column_stats(sample_df, col)\n",
    "        full_stats = full_data_stats[col]\n",
    "        \n",
    "        errors = {}\n",
    "        for stat_name in [\"mean\", \"stddev\"]:\n",
    "            errors[stat_name] = calculate_error(sample_stats[stat_name], full_stats[stat_name])\n",
    "        \n",
    "        comparison_results[sample_name][col] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier visualization\n",
    "comparison_data = []\n",
    "\n",
    "for sample_name, cols in comparison_results.items():\n",
    "    for col_name, errors in cols.items():\n",
    "        for stat_name, error_val in errors.items():\n",
    "            comparison_data.append({\n",
    "                \"Sampling Method\": sample_name,\n",
    "                \"Column\": col_name,\n",
    "                \"Statistic\": stat_name,\n",
    "                \"Error Percentage\": error_val\n",
    "            })\n",
    "\n",
    "# Create the DataFrame from the list of dictionaries\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display the comparison results\n",
    "print(\"\\nError percentages for different sampling methods compared to full dataset:\")\n",
    "pd_comparison = comparison_df.pivot_table(\n",
    "    index=[\"Sampling Method\", \"Column\"],\n",
    "    columns=\"Statistic\",\n",
    "    values=\"Error Percentage\"\n",
    ")\n",
    "print(pd_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average error by sampling method\n",
    "avg_errors = comparison_df.groupby(\"Sampling Method\")[\"Error Percentage\"].mean().reset_index()\n",
    "avg_errors = avg_errors.sort_values(\"Error Percentage\")\n",
    "print(\"\\nAverage error by sampling method (lower is better):\")\n",
    "print(avg_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Visualize comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Convert a Spark DataFrame to Pandas for visualization\n",
    "def convert_to_pandas(spark_df, columns=None, index_col=None):\n",
    "    \"\"\"Convert Spark DataFrame to Pandas DataFrame, with optional column selection\"\"\"\n",
    "    if columns:\n",
    "        return spark_df.select(columns + [index_col]).toPandas()\n",
    "    else:\n",
    "        return spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Visualize distributions for a specific column\n",
    "def plot_distributions(column, full_df, sample_dfs):\n",
    "    \"\"\"\n",
    "    Plot distribution comparisons between full data and samples\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    column : str\n",
    "        Column name to plot\n",
    "    full_df : pandas.DataFrame\n",
    "        Full dataset (pandas)\n",
    "    sample_dfs : dict\n",
    "        Dictionary of sample dataframes (pandas)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Plot the full data distribution\n",
    "    sns.kdeplot(full_df[column], label=\"Full Dataset\", color=\"black\", linewidth=2)\n",
    "    \n",
    "    # Plot each sample distribution\n",
    "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown']\n",
    "    for i, (name, df) in enumerate(sample_dfs.items()):\n",
    "        sns.kdeplot(df[column], label=f\"{name} Sample\", color=colors[i % len(colors)])\n",
    "    \n",
    "    plt.title(f\"Distribution Comparison for {column}\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Visualize time series for a specific column\n",
    "def plot_time_series(column, full_df, sample_dfs, sample_size=1000, index_col=\"utc_timestamp\"):\n",
    "    \"\"\"\n",
    "    Plot time series comparisons between full data and samples\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    column : str\n",
    "        Column name to plot\n",
    "    full_df : pandas.DataFrame\n",
    "        Full dataset (pandas)\n",
    "    sample_dfs : dict\n",
    "        Dictionary of sample dataframes (pandas)\n",
    "    sample_size : int\n",
    "        Number of points to plot (to avoid overcrowding)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Plot a sample of the full data\n",
    "    full_sample = full_df.sample(sample_size)\n",
    "    plt.plot(full_sample[index_col], full_sample[column], 'k-', alpha=0.5, label=\"Full Dataset\")\n",
    "    \n",
    "    # Plot each sample\n",
    "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown']\n",
    "    for i, (name, df) in enumerate(sample_dfs.items()):\n",
    "        # Take a subset to avoid overcrowding the plot\n",
    "        if len(df) > sample_size:\n",
    "            df_sample = df.sample(sample_size)\n",
    "        else:\n",
    "            df_sample = df\n",
    "        \n",
    "        plt.scatter(df_sample[index_col], df_sample[column], alpha=0.6, \n",
    "                   s=15, label=f\"{name} Sample\", color=colors[i % len(colors)])\n",
    "    \n",
    "    plt.title(f\"Time Series Comparison for {column}\")\n",
    "    plt.xlabel(index_col)\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Compare error distributions across sampling methods\n",
    "def plot_error_comparison(comparison_df):\n",
    "    \"\"\"Plot error comparison across sampling methods\"\"\"\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    sns.boxplot(x=\"Sampling Method\", y=\"Error Percentage\", data=comparison_df)\n",
    "    \n",
    "    plt.title(\"Error Distribution by Sampling Method\")\n",
    "    plt.xlabel(\"Sampling Method\")\n",
    "    plt.ylabel(\"Error Percentage\")\n",
    "    plt.yscale('log')  # Log scale for better visibility of differences\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 Compare specific statistics across methods\n",
    "def plot_statistic_comparison(comparison_df, statistic=\"mean\"):\n",
    "    \"\"\"Plot comparison of a specific statistic across sampling methods\"\"\"\n",
    "    stat_df = comparison_df[comparison_df[\"Statistic\"] == statistic]\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    sns.barplot(x=\"Sampling Method\", y=\"Error Percentage\", data=stat_df)\n",
    "    \n",
    "    plt.title(f\"{statistic.capitalize()} Error by Sampling Method\")\n",
    "    plt.xlabel(\"Sampling Method\")\n",
    "    plt.ylabel(f\"{statistic.capitalize()} Error Percentage\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to pandas for visualization\n",
    "print(\"Converting data to pandas for visualization...\")\n",
    "# Use a sample of full data to avoid memory issues\n",
    "full_data_pd = convert_to_pandas(full_df_de, selected_columns, \"utc_timestamp\")\n",
    "\n",
    "sample_dfs = {}\n",
    "for name, df in samples.items():\n",
    "    sample_dfs[name] = convert_to_pandas(df, selected_columns, \"utc_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions\n",
    "for column in selected_columns[:1]:\n",
    "    plot_distributions(column, full_data_pd, sample_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series\n",
    "for column in selected_columns[:1]:\n",
    "    plot_time_series(column, full_data_pd, sample_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error comparisons\n",
    "plot_error_comparison(comparison_df)\n",
    "plot_statistic_comparison(comparison_df, \"mean\")\n",
    "plot_statistic_comparison(comparison_df, \"stddev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Recomended Sampling Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best sampling method based on lowest average error\n",
    "best_method = avg_errors.iloc[0][\"Sampling Method\"]\n",
    "print(f\"\\nRecommended sampling method: {best_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide recommendations based on error analysis\n",
    "recommended_sample_size = sample_size\n",
    "\n",
    "# If the best method is adaptive sampling, use its adjusted sample size\n",
    "if best_method == \"Adaptive\":\n",
    "    recommended_sample_size = adaptive_sample.count()\n",
    "\n",
    "print(f\"Recommended sample size: {recommended_sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add recommendations based on performance analysis\n",
    "print(\"\\nRecommendations based on performance analysis:\")\n",
    "print(\"1. Use the selected sampling method for your EDA and statistical analysis\")\n",
    "print(f\"2. The sample size of {recommended_sample_size} should provide estimates with a margin of error \"\n",
    "      f\"of approximately {MARGIN_OF_ERROR*100}% at a {CONFIDENCE_LEVEL*100}% confidence level\")\n",
    "print(\"3. For time-critical visualizations and EDA tasks, this sample provides a good balance \"\n",
    "      \"between accuracy and computational efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional recommendations based on data characteristics\n",
    "print(\"\\nAdditional recommendations:\")\n",
    "print(\"- For time series pattern detection, ensure your sampling preserves the temporal patterns\")\n",
    "print(\"- For anomaly detection, systematic or stratified sampling are recommended\")\n",
    "print(\"- For correlation analysis between sensors, ensure sample represents the full feature space\")\n",
    "print(\"- Consider using different sampling strategies for different analysis tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nImplementing the recommended sampling strategy for production...\")\n",
    "\n",
    "# Function to apply the best sampling method\n",
    "def apply_best_sampling(data, sample_size, best_method):\n",
    "    \"\"\"Apply the best sampling method determined by analysis\"\"\"\n",
    "    \n",
    "    if best_method == \"Random\":\n",
    "        return random_sampling(data, sample_size)\n",
    "    elif best_method == \"Systematic\":\n",
    "        return systematic_sampling(data, sample_size)\n",
    "    elif best_method == \"Stratified\":\n",
    "        return stratified_time_sampling(data, sample_size)\n",
    "    elif best_method == \"Cluster\":\n",
    "        return cluster_sampling(data, sample_size)\n",
    "    elif best_method == \"Reservoir\":\n",
    "        return reservoir_sampling_simulation(data, sample_size)\n",
    "    elif best_method == \"Adaptive\":\n",
    "        return adaptive_sampling(data, sample_size, sample_size*5)\n",
    "    else:\n",
    "        # Default to random sampling\n",
    "        return random_sampling(data, sample_size)\n",
    "\n",
    "# Sample implementation for production\n",
    "def production_sampling_and_analysis(data, best_method, confidence_level=0.95, margin_of_error=0.01):\n",
    "    \"\"\"\n",
    "    Production function for sampling and analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pyspark.sql.DataFrame\n",
    "        Input time series data\n",
    "    confidence_level : float\n",
    "        Desired confidence level (default: 0.95)\n",
    "    margin_of_error : float\n",
    "        Desired margin of error (default: 0.03)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pyspark.sql.DataFrame\n",
    "        Sampled data\n",
    "    \"\"\"\n",
    "    # Calculate required sample size\n",
    "    sample_size = calculate_sample_size(confidence_level, margin_of_error)\n",
    "    \n",
    "    # Apply the best sampling method\n",
    "    sampled_data = apply_best_sampling(data, sample_size, best_method)\n",
    "    \n",
    "    print(f\"Applied {best_method} sampling with size {sampled_data.count()}\")\n",
    "    print(f\"Expected error: Â±{margin_of_error*100}% at {confidence_level*100}% confidence level\")\n",
    "    \n",
    "    return sampled_data\n",
    "\n",
    "# Example usage\n",
    "final_sample = production_sampling_and_analysis(full_df_de, best_method)\n",
    "print(f\"Final sample size: {final_sample.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_de.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "def analyze_missing_values(df, output_dir=None):\n",
    "    \"\"\"\n",
    "    Analyze missing values in the DataFrame efficiently.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : PySpark DataFrame\n",
    "        DataFrame to analyze\n",
    "    output_dir : str, optional\n",
    "        Directory to save the results. If None, results are not saved.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Missing values statistics\n",
    "    \"\"\"\n",
    "    # Calculate total count of rows\n",
    "    total_count = df.count()\n",
    "    \n",
    "    # Compute missing counts for all columns in one pass\n",
    "    missing_counts = df.select(\n",
    "        [spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]\n",
    "    ).collect()[0]\n",
    "    \n",
    "    # Create a list to hold missing value information\n",
    "    missing_list = []\n",
    "    \n",
    "    for column_name in df.columns:\n",
    "        missing_count = missing_counts[column_name]\n",
    "        missing_percentage = (missing_count / total_count) * 100\n",
    "        \n",
    "        missing_list.append({\n",
    "            \"Column\": column_name,\n",
    "            \"Missing Count\": missing_count,\n",
    "            \"Missing Percentage\": missing_percentage\n",
    "        })\n",
    "    \n",
    "    # Convert to pandas DataFrame for easier visualization\n",
    "    missing_df = pd.DataFrame(missing_list)\n",
    "    missing_df = missing_df.sort_values(\"Missing Percentage\", ascending=False)\n",
    "    \n",
    "    # Save missing values information if output_dir is provided\n",
    "    if output_dir:\n",
    "        missing_df.to_csv(f\"{output_dir}/missing_values.csv\", index=False)\n",
    "    \n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_missing_values(full_df_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_missing_values_between(missing_df, min_percentage, max_percentage):\n",
    "    \"\"\"\n",
    "    Plot a bar chart of missing values percentages for columns with missing percentage between min_percentage and max_percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    missing_df : pd.DataFrame\n",
    "        DataFrame containing missing values statistics with columns 'Column', 'Missing Count', and 'Missing Percentage'\n",
    "    min_percentage : float\n",
    "        Minimum missing percentage to filter columns\n",
    "    max_percentage : float\n",
    "        Maximum missing percentage to filter columns\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame based on the given percentage range\n",
    "    filtered_df = missing_df[(missing_df['Missing Percentage'] >= min_percentage) & (missing_df['Missing Percentage'] <= max_percentage)]\n",
    "    \n",
    "    # Plot using seaborn barplot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.barplot(x='Missing Percentage', y='Column', data=filtered_df, palette='viridis')\n",
    "    plt.title(f'Missing Values Percentage by Column (Between {min_percentage}% and {max_percentage}%)')\n",
    "    plt.xlabel('Missing Percentage')\n",
    "    plt.ylabel('Column')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "missing_df = analyze_missing_values(full_df_de)\n",
    "plot_missing_values_between(missing_df, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function 1: Create a boolean mask for missing values\n",
    "def create_missing_mask(df):\n",
    "    \"\"\"\n",
    "    Create a boolean mask indicating missing values in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : PySpark DataFrame\n",
    "        The input DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    PySpark DataFrame\n",
    "        A DataFrame with the same structure as `df`, where each value is `True` (missing) or `False` (not missing).\n",
    "    \"\"\"\n",
    "    return df.select([col(c).isNull().alias(c) for c in df.columns])\n",
    "\n",
    "# Function 2: Collect the mask and convert it to a NumPy array\n",
    "def mask_to_numpy_array(missing_mask):\n",
    "    \"\"\"\n",
    "    Collect the boolean mask and convert it to a NumPy array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    missing_mask : PySpark DataFrame\n",
    "        The boolean mask DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        A NumPy array representing the boolean mask.\n",
    "    \"\"\"\n",
    "    # Collect the mask as a list of rows\n",
    "    missing_data = missing_mask.collect()\n",
    "    \n",
    "    # Convert the collected data to a NumPy array\n",
    "    return np.array([list(row) for row in missing_data], dtype=bool)\n",
    "\n",
    "# Function 3: Visualize the missing values using matplotlib\n",
    "def visualize_missing_values(missing_array):\n",
    "    \"\"\"\n",
    "    Visualize the missing values using a heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    missing_array : np.ndarray\n",
    "        A NumPy array representing the boolean mask.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.imshow(missing_array, aspect=\"auto\", interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.xlabel(\"Column Number\")\n",
    "    plt.ylabel(\"Sample Number\")\n",
    "    plt.title(\"Missing Values Visualization\")\n",
    "    plt.colorbar(label=\"Missing (True) / Not Missing (False)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mask = create_missing_mask(full_df_de)\n",
    "missing_array = mask_to_numpy_array(missing_mask)\n",
    "visualize_missing_values(missing_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count\n",
    "\n",
    "def detect_duplicates_optimized(df, columns, output_dir=None):\n",
    "    \"\"\"\n",
    "    Detect duplicates in a PySpark DataFrame efficiently and provide additional insights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : PySpark DataFrame\n",
    "        DataFrame to analyze.\n",
    "    columns : list\n",
    "        List of column names to check for duplicates.\n",
    "    output_dir : str, optional\n",
    "        Directory to save the results. If None, results are not saved.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        - PySpark DataFrame: Full duplicate rows.\n",
    "        - PySpark DataFrame: Summary of duplicate counts per group.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no columns are specified, use all columns for partitioning\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    \n",
    "    # Define a window partitioned by the specified or all columns\n",
    "    window = Window.partitionBy(columns)\n",
    "    \n",
    "    # Add a count column to identify duplicates\n",
    "    df_with_count = df.withColumn(\"duplicate_count\", count(\"*\").over(window))\n",
    "    \n",
    "    # Filter rows where duplicate_count > 1 (duplicates)\n",
    "    duplicates_full = df_with_count.filter(col(\"duplicate_count\") > 1).drop(\"duplicate_count\")\n",
    "    \n",
    "    # Create a summary of duplicate counts per group\n",
    "    duplicates_summary = df_with_count.groupBy(columns).agg(\n",
    "        count(\"*\").alias(\"duplicate_count\")\n",
    "    ).filter(\"duplicate_count > 1\")\n",
    "    \n",
    "    # Save results if output_dir is provided\n",
    "    if output_dir:\n",
    "        duplicates_full.write.csv(f\"{output_dir}/duplicates_full\", header=True, mode=\"overwrite\")\n",
    "        duplicates_summary.write.csv(f\"{output_dir}/duplicates_summary\", header=True, mode=\"overwrite\")\n",
    "    \n",
    "    return duplicates_full, duplicates_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_duplicates_optimized(full_df_de, full_df_de.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
